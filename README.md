# Learning-Optimal-Lunar-Landings-A-PPO-Clip-with-Beta-Distribution-Approach

In this project, I explore the application of Proximal Policy Optimization (PPOClip) with a Beta distribution [Ref.1] to train an agent in the Lunar Lander v3
environment with a continuous action space. Traditional reinforcement learning
algorithms often struggle with continuous action spaces due to the complexity of
exploration and policy representation. By leveraging a Beta distribution, my policy
network can effectively model bounded action spaces, improving stability and performance. Compared to Deep Deterministic Policy Gradient (DDPG), PPO-Clip
offers a more stable training process by avoiding the overestimation bias associated with Q-learning-based methods. My approach integrates deep reinforcement
learning techniques with function approximation using a neural network. The results demonstrate the efficacy of PPO-Clip in learning optimal landing strategies,
highlighting the importance of proper action distribution modeling in reinforcement
learning.

<img width="695" alt="image" src="https://github.gatech.edu/emohagheghian3/Learning-Optimal-Lunar-Landings-A-PPO-Clip-with-Beta-Distribution-Approach/assets/93353/50b1e111-1913-4a08-a974-bce3f2c80d6b">

<img width="697" alt="image" src="https://github.gatech.edu/emohagheghian3/Learning-Optimal-Lunar-Landings-A-PPO-Clip-with-Beta-Distribution-Approach/assets/93353/ffa7a682-7bc7-4b68-ba35-fc7f4c33b3fa">

<img width="697" alt="image" src="https://github.gatech.edu/emohagheghian3/Learning-Optimal-Lunar-Landings-A-PPO-Clip-with-Beta-Distribution-Approach/assets/93353/c7d95d70-7ee9-4a10-9e4c-05bb76ce3252">

